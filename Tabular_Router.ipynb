{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "3d50f6b3",
			"metadata": {},
			"source": [
				"# WORK IN PROGRESS \n",
				"# LLM Pipeline Routing\n",
				"\n",
				"In this notebook we will show how to use routing to different LLM pipelines as well as how to work with tabular data.\n",
				"\n",
				"We will consider on Tesla (TSLA) 10k report and stock prices data. \n",
				"* If a question pertains to the 10k, we will route to a vanilla RAG\n",
				"* If a question pertains to the stock price, we will route to the tabular data GenAI path"
			]
		},
		{
			"cell_type": "markdown",
			"id": "a23f9e06",
			"metadata": {},
			"source": [
				"# Import libraries and load the 10k and stock data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"id": "64e51b8d4fcea0e1deb7dcf3",
			"metadata": {
				"chart_settings": null,
				"collapsed": null,
				"custom_metric_settings": null,
				"dataframe_view_options": null,
				"datarobot": {
					"language": "python"
				},
				"disable_run": null,
				"hide_code": null,
				"hide_results": null,
				"scrolled": null
			},
			"outputs": [],
			"source": [
				"import subprocess\n",
				"import tiktoken\n",
				"import pandas as pd\n",
				"import os\n",
				"import csv\n",
				"import json\n",
				"import time\n",
				"import re\n",
				"import transformers\n",
				"import torch\n",
				"import numpy as np\n",
				"\n",
				"# To use with the router\n",
				"from rouge_score import rouge_scorer\n",
				"from nltk.corpus import stopwords\n",
				"from nltk.stem import WordNetLemmatizer\n",
				"from nltk.tokenize import word_tokenize\n",
				"from sklearn.metrics.pairwise import cosine_similarity\n",
				"\n",
				"#We will use langchanin to create a vector store to retrieve stronger negatives\n",
				"from langchain.vectorstores.faiss import FAISS\n",
				"from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
				"from langchain.document_loaders import UnstructuredPDFLoader, csv_loader \n",
				"# from langchain.embeddings.sentence_transformer import HuggingFaceEmbeddings\n",
				"from langchain_community.embeddings import HuggingFaceEmbeddings\n",
				"\n",
				"\n",
				"EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"## \"BAAI/bge-base-en-v1.5\"\"all-MiniLM-L6-v2\" "
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"id": "71be0425",
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"Document(page_content='Date: 2010-06-29\\nOpen: 1.266667\\nHigh: 1.666667\\nLow: 1.169333\\nClose: 1.592667\\nAdj Close: 1.592667\\nVolume: 281494500', metadata={'source': './data/TSLA.csv', 'row': 0})"
						]
					},
					"execution_count": 15,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"# read and load 10k pdf file\n",
				"loader = UnstructuredPDFLoader(\"./data/tsla-20231231-gen.pdf\")\n",
				"docs_10k = loader.load()\n",
				"\n",
				"#load stock data. Load dataframe and load directly as docs\n",
				"tsla_stock = pd.read_csv(\"./data/TSLA.csv\")\n",
				"loader  = csv_loader.CSVLoader(file_path=\"./data/TSLA.csv\")\n",
				"data = loader.load()\n",
				"data[0]"
			]
		},
		{
			"cell_type": "markdown",
			"id": "c6e5a9c4",
			"metadata": {},
			"source": [
				"# Split docs and store in FAISS Vector Database for 10k"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"id": "353df606",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"2530\n"
					]
				}
			],
			"source": [
				"#let's split the document into chunks chunk size 128 and overlap size 64\n",
				"text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 128, chunk_overlap  = 64, add_start_index = True)\n",
				"docs_split = text_splitter.split_documents(docs_10k)\n",
				"print(len(docs_split))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"id": "4433b705",
			"metadata": {},
			"outputs": [
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "41d9528438c0454f8f9a9bb87fc7e92b",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "d42ad259c058489a860a452af41b7788",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "5e6ef495fcfe40658b8484b8bf7e8994",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "f839f0b488034068a353d5cbfd8e05a2",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "efda047af2bf4ca093b1868356ca683d",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "5cde2d749a7c48d1a800991298207e5e",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "71627dce33684f74854bbfa8020f64ff",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "7296119c43394e579dbf0c28d0ec3572",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "e2e16c57d37a407db87ca7c488df0548",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "f42ccac3672942d8bb94d70263e6c554",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "959ffa20ffb34b498b09fb6c2ca41c27",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "a7a918c2e4e34e8396611a240fc95e61",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "8168d27fc3974091abad57532ace2406",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "d5a6fe742be74474ace2413bf7624ff8",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				}
			],
			"source": [
				"embedding_function = HuggingFaceEmbeddings(\n",
				"        model_name=EMBEDDING_MODEL_NAME,\n",
				"        cache_folder=\"./models/sentencetransformers\"\n",
				"    )"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"id": "e1f66b9d",
			"metadata": {},
			"outputs": [],
			"source": [
				"db = FAISS.from_documents(docs_split, embedding_function)\n",
				"db.save_local(\"./data/faiss\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "a3913892",
			"metadata": {},
			"source": [
				"# Build Static Knowledge Base of Stock Data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "4a26a61e",
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"id": "e3693235",
			"metadata": {},
			"source": [
				"# Create CSV Agent"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"id": "22fecec8",
			"metadata": {},
			"outputs": [],
			"source": []
		},
		{
			"cell_type": "markdown",
			"id": "5d458c7d",
			"metadata": {},
			"source": [
				"# Build Question Bank and Similarity Functions to Help Route Questions"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 17,
			"id": "cbfca7d6",
			"metadata": {},
			"outputs": [],
			"source": [
				"COMPANY_DETAILS_QUESTIONS = [\"Describe {company_name}'s business?\", \"What sector or industry does {company_name} operate in?\", \"What market does {company_name} serve?\",\n",
				"                             \"What products does {company_name} offer?\", \"What services does the {company_name} offer?\", \"Who are the {company_name}'s clients or customers?\", \n",
				"                             \"Who are the {company_name}'s suppliers?\", \"Who are the {company_name}'s vendors?\", \"Who are the {company_name}'s competitors?\"]\n",
				"\n",
				"#use equity research questions (for other research questions, we can use the same format)\n",
				"COVERAGE_QUESTIONS = [\"What is the analyst coverage for {company_name}?\", \"What is the analyst rating for {company_name}?\", \"What is the analyst price target for {company_name}?\"]\n",
				"\n",
				"FINANCIALS_QUESTIONS = [\"What is the revenue for {company_name}?\", \"What is the net income for {company_name}?\",\"What is the operating income for {company_name}?\",\"What's {company_name}'s EBITDA?\", \n",
				"                        \"What is the gross profit for {company_name}?\", \"What's {company_name}'s gross margin like?\", \"How much cash does {company_name} have?\", \"How much revenue did {company_name} earn in the last quarter?\"]\n",
				"\n",
				"\n",
				"QUESTION_LISTS = [COMPANY_DETAILS_QUESTIONS, COVERAGE_QUESTIONS, FINANCIALS_QUESTIONS]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 18,
			"id": "8a652ab1",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize the lemmatizer and get list of stop words\n",
				"lemmatizer = WordNetLemmatizer()\n",
				"stop_words = set(stopwords.words('english'))\n",
				"\n",
				"def get_max_rouge_f1(prompt, question_list, company_name, lower=True):\n",
				"    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
				"    if lower:\n",
				"        prompt = prompt.lower()\n",
				"        question_list = [question.lower() for question in question_list]\n",
				"    words = word_tokenize(prompt)\n",
				"    # Remove stop words and lemmatize the words\n",
				"    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
				"    prompt_clean = ' '.join(words)\n",
				"\n",
				"    question_list_clean=[]\n",
				"    max_score = 0\n",
				"    best_question = ''\n",
				"    for question in question_list:\n",
				"        question = question.format(company_name=company_name)\n",
				"        words = word_tokenize(question)\n",
				"        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
				"        question_clean = ' '.join(words)\n",
				"        question_list_clean.append(question_clean)\n",
				"        scores = scorer.score(prompt_clean, question_clean)\n",
				"        if scores['rouge1'].fmeasure > max_score:\n",
				"            max_score = scores['rouge1'].fmeasure\n",
				"            best_question = question\n",
				"    return max_score, best_question"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 19,
			"id": "a5a691a4",
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_max_similarity(prompt, question_list, company_name):\n",
				"\n",
				"    prompt_embedding = embedding_function.embed_documents([prompt])\n",
				"    question_list = [question.format(company_name=company_name) for question in question_list]\n",
				"    question_list_embeddings = embedding_function.embed_documents(question_list)\n",
				"\n",
				"    similarity = cosine_similarity(prompt_embedding, question_list_embeddings)\n",
				"\n",
				"    max_score = np.max(similarity)\n",
				"    best_question = question_list[np.argmax(similarity)]\n",
				"    return max_score, best_question"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 20,
			"id": "5cce2586",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"company: (0.4, \"who are the Tesla's suppliers?\")\n",
						"FINANCIALS_QUESTIONS: (0.4615384615384615, 'how much revenue did Tesla earn in the last quarter?')\n",
						"COVERAGE_QUESTIONS: (0.2, 'what is the analyst coverage for Tesla?')\n"
					]
				}
			],
			"source": [
				"question = \"How much was Tesla's revenue for the 2023 fiscal year?\"\n",
				"\n",
				"print(\"company: {}\".format(get_max_rouge_f1(question, COMPANY_DETAILS_QUESTIONS, company_name=\"Tesla\")))\n",
				"print(\"FINANCIALS_QUESTIONS: {}\".format(get_max_rouge_f1(question, FINANCIALS_QUESTIONS, company_name=\"Tesla\")))\n",
				"print(\"COVERAGE_QUESTIONS: {}\".format(get_max_rouge_f1(question, COVERAGE_QUESTIONS, company_name=\"Tesla\")))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f9ab5900",
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "streamlit",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.8.10"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
